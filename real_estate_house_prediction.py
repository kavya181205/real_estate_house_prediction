# -*- coding: utf-8 -*-
"""Real_EState_House_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t9vMy8aywSfOqzPKbu8TlCWBiKWAgZIF
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)

"""### **Loading Dataset**
- This cell loads the dataset from a file CSV into a pandas DataFrame for further analysis.

"""

data=pd.read_csv('bengaluru_house_prices.csv')
data

data.shape

data.columns

df = data.drop(['area_type','society','balcony','availability'],axis='columns')
df.shape

df

"""# Data Cleaning: Handle NA values"""

df.isnull().sum()

df['size'].unique()

df=df.dropna()

df

df.shape

df.isnull().sum()

"""# Feature Engineering"""

df['bhk']=df['size'].apply(lambda x:int(x.split(' ')[0]))
df.bhk.unique()

df

df['bhk'].isnull().sum()

df['total_sqft'].unique()

def convert_sqft_to_num(x):
    tokens = x.split('-')
    if len(tokens) == 2:
        return (float(tokens[0])+float(tokens[1]))/2
    try:
        return float(x)
    except:
        return None

df2 = df.copy()
df2.total_sqft = df2.total_sqft.apply(convert_sqft_to_num)
df2 = df2[df2.total_sqft.notnull()]
df2

"""**We create a new column `price_per_sqft` to calculate the price of each property per square foot.**  """

df2['price_per_sqrt']=df2['price']*100000/df2['total_sqft']
df2

location=df2['location'].value_counts(ascending= False)
location

len(location[location>10])

"""- To reduce the dimensionality of the `location` feature, we group infrequent locations (those with less than or equal to 10 occurrences) into a single - category called **"other"**.  
- This helps avoid the curse of dimensionality and improves model generalization.  


"""

location=location[location<=10]
location

df2.location = df2.location.apply(lambda x: 'other' if x in location else x)
len(df2.location.unique())

df2

df2.shape

df2[df2['total_sqft']/df2['bhk']<300]

df2=df2[~(df2['total_sqft']/df2['bhk']<300)]
df2

df2['price_per_sqrt'].describe()

"""### **Removing Outliers Based on Price per Square Foot (PPS)**

Outliers in real estate data often occur when properties are abnormally priced compared to similar ones in the same location.  
To handle this, we define a function `remove_pps_outliers()` which:

1. Groups the dataset by **location**.  
2. Computes the **mean** and **standard deviation** of `price_per_sqrt` for each location.  
3. Keeps only those rows where the price per square foot lies within **(mean - std, mean + std)**.  
   - This effectively removes extreme low or high values in each location.  
4. Concatenates the cleaned sub-dataframes into one and returns it.  

Finally, we check the new shape of the dataset after outlier removal.

"""

def remove_pps_outliers(df):
    df_out = pd.DataFrame()
    for key, subdf in df.groupby('location'):
        m = np.mean(subdf.price_per_sqrt)
        st = np.std(subdf.price_per_sqrt)
        reduced_df = subdf[(subdf.price_per_sqrt>(m-st)) & (subdf.price_per_sqrt<=(m+st))]
        df_out = pd.concat([df_out,reduced_df],ignore_index=True)
    return df_out
df3 = remove_pps_outliers(df2)
df3.shape

df3

"""### **Price vs Total Square Feet (Before Outlier Removal) – Rajaji Nagar**

To visualize the presence of outliers, we plot the **Price vs Total Square Feet** for properties in the *Rajaji Nagar* location **before** outlier removal.

"""

def plot_scatter_chart(df,location):
    bhk2 = df[(df.location==location) & (df.bhk==2)]
    bhk3 = df[(df.location==location) & (df.bhk==3)]
    matplotlib.rcParams['figure.figsize'] = (15,10)
    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK', s=50)
    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+', color='green',label='3 BHK', s=50)
    plt.xlabel("Total Square Feet Area")
    plt.ylabel("Price (Lakh Indian Rupees)")
    plt.title(location)
    plt.legend()

plot_scatter_chart(df3,"Rajaji Nagar")

"""### **Price vs Total Square Feet (Before Outlier Removal) – Hebbal**

To visualize the presence of outliers, we plot the **Price vs Total Square Feet** for properties in the *Rajaji Nagar* location **before** outlier removal.

"""

plot_scatter_chart(df3,'Hebbal')

"""### **Outlier Removal Based on BHK and Location**

We also remove outliers by comparing prices across **BHK levels within the same location**.  
For example:
- A 3 BHK property in a given location should not have a lower price per square foot than the **average 2 BHK property** in the same area.
- If it does, such properties are treated as outliers and removed.

This ensures price progression with higher BHKs is consistent and realistic.

"""

import numpy as np

def remove_outliers(df):
    exclude_indices = np.array([])

    for location, location_df in df.groupby('location'):
        bhk_stats = {}
        for bhk, bhk_df in location_df.groupby('bhk'):
            bhk_stats[bhk] = {
                'mean': np.mean(bhk_df.price_per_sqrt),
                'std': np.std(bhk_df.price_per_sqrt),
                'count': bhk_df.shape[0]
            }
        for bhk, bhk_df in location_df.groupby('bhk'):
            stats = bhk_stats.get(bhk - 1)
            if stats and stats['count'] > 5:
                outliers = bhk_df[bhk_df.price_per_sqrt < stats['mean']]
                exclude_indices = np.append(exclude_indices, outliers.index.values)

    return df.drop(exclude_indices, axis='index')

# Apply to your DataFrame
df3 = remove_outliers(df3)
print(df3.shape)

df3

"""### **Price vs Total Square Feet (After Outlier Removal) – Rajaji Nagar**

To visualize the presence of outliers, we plot the **Price vs Total Square Feet** for properties in the *Rajaji Nagar* location **After** outlier removal.

"""

plot_scatter_chart(df3,"Rajaji Nagar")

"""### **Price vs Total Square Feet (After Outlier Removal) – Hebbal**

To visualize the presence of outliers, we plot the **Price vs Total Square Feet** for properties in the *Rajaji Nagar* location **After** outlier removal.

"""

plot_scatter_chart(df3,'Hebbal')

df3=df3[df3.bath<10]
df3

df3[df3.bath>df3.bhk+2]

df4=df3[df3.bath<df3.bhk+2]
df4

final=df4.drop(['size','price_per_sqrt'],axis='columns')
final

"""### **One-Hot Encoding for Location**

The `location` column is categorical, so we convert it into dummy variables using **one-hot encoding**.  
- Each location becomes a separate binary column (`1` if the property belongs to that location, otherwise `0`).  
- To avoid the **dummy variable trap** (perfect multicollinearity), we drop the `"other"` column.  

This transforms the dataset into a fully numeric format suitable for machine learning models.

"""

dummies=pd.get_dummies(final.location)
dummies

final_df=pd.concat([final,dummies.drop('other',axis=1)],axis=1)
final_df

final = final_df.drop('location',axis='columns')
final

X=final.drop('price',axis=1)
y=final['price']

X

y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

"""### **Model Selection using GridSearchCV**

To identify the best algorithm for predicting house prices, we test multiple models with hyperparameter tuning:

1. **Linear Regression**
   - Baseline model with no hyperparameters to tune.

2. **Lasso Regression**
   - Useful for feature selection and regularization.
   - Tuned parameters:
     - `alpha`: Regularization strength.
     - `selection`: Coordinate descent strategy (`random` or `cyclic`).

3. **Decision Tree Regressor**
   - Captures non-linear relationships.
   - Tuned parameters:
     - `criterion`: Function to measure split quality (`squared_error`, `friedman_mse`).
     - `splitter`: Strategy for splitting (`best`, `random`).

We use **ShuffleSplit Cross Validation** (5 splits, 20% test size) to evaluate the performance of each algorithm.  
The results are collected in a DataFrame containing:
- Model name
- Best cross-validation score
- Best hyperparameters

"""

from sklearn.linear_model import LinearRegression, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV, ShuffleSplit
import pandas as pd

def find_best_model_using_gridsearchcv(X, y):
    algos = {
        'linear_regression': {
            'model': LinearRegression(),
            'params': {
            }
        },
        'lasso': {
            'model': Lasso(),
            'params': {
                'alpha': [1, 2],
                'selection': ['random', 'cyclic']
            }
        },
        'decision_tree': {
            'model': DecisionTreeRegressor(),
            'params': {
                'criterion': ['squared_error', 'friedman_mse'],
                'splitter': ['best', 'random']
            }
        }
    }

    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

    for algo_name, config in algos.items():
        if not config['params']:  # Skip GridSearchCV if no hyperparameters to tune
            model = config['model']
            model.fit(X, y)
            score = model.score(X, y)
            scores.append({
                'model': algo_name,
                'best_score': score,
                'best_params': 'Default parameters'
            })
        else:
            gs = GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
            gs.fit(X, y)
            scores.append({
                'model': algo_name,
                'best_score': gs.best_score_,
                'best_params': gs.best_params_
            })

    return pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])

find_best_model_using_gridsearchcv(X,y)

def predict_price(location,sqft,bath,bhk):
    loc_index = np.where(X.columns==location)[0][0]

    x = np.zeros(len(X.columns))
    x[0] = sqft
    x[1] = bath
    x[2] = bhk
    if loc_index >= 0:
        x[loc_index] = 1

    return lr.predict([x])[0]

predict_price('1st Phase JP Nagar',1000, 2, 2)

predict_price('1st Phase JP Nagar',1000, 3, 3)

predict_price('Indira Nagar',1000, 2, 2)

predict_price('Indira Nagar',1000, 3, 3)

"""###  **Conclusion**

- We evaluated **Linear Regression**, **Lasso Regression**, and **Decision Tree Regressor** using **GridSearchCV** with ShuffleSplit cross-validation.
- Based on the comparison:
  - **Linear Regression** performed the best (highest cross-validation score).
  - Lasso was useful for feature selection but did not outperform Linear Regression.
  - Decision Tree showed lower performance due to possible overfitting on training data.
- Therefore, **Linear Regression** is chosen as the final model for predicting Bangalore house prices.
- This model generalizes well and balances accuracy with simplicity, making it suitable for deployment.

"""